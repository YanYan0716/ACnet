{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"TPU版ACNET：\n实现未在原数据集，这里使用的是TPU中一个104种花朵分类的数据集\nstage1：acc=89%左右\n未曾仔细调参，只是证明代码可收敛并在TPU上可使用\n可直接执行全部代码，前提是需在kaggle下的jupyter才能使用数据集\n\n\n 细粒度分类\n \n reference：\n \n https://isrc.iscas.ac.cn/gitlab/research/acnet\n \n https://arxiv.org/pdf/1909.11378.pdf\n \n tensorflow==2.3.0\n \n \n 基本完全按照原文的思路实现的树形结构，但在参数设置、各个模块的实现细节等方面有参照以下\n \n https://github.com/FlyingMoon-GitHub/ACNet","metadata":{}},{"cell_type":"code","source":"import math, re, os\nimport tensorflow as tf\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-16T09:37:49.782766Z","iopub.execute_input":"2021-08-16T09:37:49.783114Z","iopub.status.idle":"2021-08-16T09:37:49.790902Z","shell.execute_reply.started":"2021-08-16T09:37:49.783085Z","shell.execute_reply":"2021-08-16T09:37:49.789770Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"Tensorflow version 2.4.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# TPU or GPU detection","metadata":{}},{"cell_type":"code","source":"# NEW on TPU in TensorFlow 24: shorter cross-compatible TPU/GPU/multi-GPU/cluster-GPU detection code\n\ntry: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # detect GPUs\n    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    #strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:37:53.611317Z","iopub.execute_input":"2021-08-16T09:37:53.611679Z","iopub.status.idle":"2021-08-16T09:37:59.192929Z","shell.execute_reply.started":"2021-08-16T09:37:53.611648Z","shell.execute_reply":"2021-08-16T09:37:59.191633Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"Number of accelerators:  8\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Competition data access\nTPUs read data directly from Google Cloud Storage (GCS). This Kaggle utility will copy the dataset to a GCS bucket co-located with the TPU. If you have multiple datasets attached to the notebook, you can pass the name of a specific dataset to the get_gcs_path function. The name of the dataset is the name of the directory it is mounted in. Use `!ls /kaggle/input/` to list attached datasets.","metadata":{}},{"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path() # you can list the bucket with \"!gsutil ls $GCS_DS_PATH\"","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:29:04.159535Z","iopub.execute_input":"2021-08-16T09:29:04.160175Z","iopub.status.idle":"2021-08-16T09:29:04.530746Z","shell.execute_reply.started":"2021-08-16T09:29:04.16013Z","shell.execute_reply":"2021-08-16T09:29:04.529909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = [512, 512] # At this size, a GPU will run out of memory. Use the TPU.\n                        # For GPU training, please select 224 x 224 px image size.\nEPOCHS = 12\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nPER_REPLICA_BATCH_SIZE = 16\n\nGCS_PATH_SELECT = { # available image sizes\n    192: GCS_DS_PATH + '/tfrecords-jpeg-192x192',\n    224: GCS_DS_PATH + '/tfrecords-jpeg-224x224',\n    331: GCS_DS_PATH + '/tfrecords-jpeg-331x331',\n    512: GCS_DS_PATH + '/tfrecords-jpeg-512x512'\n}\nGCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec') # predictions on this dataset should be submitted for the competition\n\nCLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                                                                                                                                               # 100 - 102","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:29:08.062922Z","iopub.execute_input":"2021-08-16T09:29:08.063261Z","iopub.status.idle":"2021-08-16T09:29:08.302267Z","shell.execute_reply.started":"2021-08-16T09:29:08.06323Z","shell.execute_reply":"2021-08-16T09:29:08.301336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization utilities\ndata -> pixels, nothing of much interest for the machine learning practitioner in this section.","metadata":{}},{"cell_type":"code","source":"# numpy and matplotlib defaults\nnp.set_printoptions(threshold=15, linewidth=80)\n\ndef batch_to_numpy_images_and_labels(data):\n    images, labels = data\n    numpy_images = images.numpy()\n    numpy_labels = labels.numpy()\n    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n        numpy_labels = [None for _ in enumerate(numpy_images)]\n    # If no labels, only image IDs, return None for labels (this is the case for test data)\n    return numpy_images, numpy_labels\n\ndef title_from_label_and_target(label, correct_label):\n    if correct_label is None:\n        return CLASSES[label], True\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct\n\ndef display_one_flower(image, title, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    if len(title) > 0:\n        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n    return (subplot[0], subplot[1], subplot[2]+1)\n    \ndef display_batch_of_images(databatch, predictions=None):\n    \"\"\"This will work with:\n    display_batch_of_images(images)\n    display_batch_of_images(images, predictions)\n    display_batch_of_images((images, labels))\n    display_batch_of_images((images, labels), predictions)\n    \"\"\"\n    # data\n    images, labels = batch_to_numpy_images_and_labels(databatch)\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n        \n    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)//rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot=(rows,cols,1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n        title = '' if label is None else CLASSES[label]\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target(predictions[i], label)\n        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()\n\ndef display_confusion_matrix(cmat, score, precision, recall):\n    plt.figure(figsize=(15,15))\n    ax = plt.gca()\n    ax.matshow(cmat, cmap='Reds')\n    ax.set_xticks(range(len(CLASSES)))\n    ax.set_xticklabels(CLASSES, fontdict={'fontsize': 7})\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n    ax.set_yticks(range(len(CLASSES)))\n    ax.set_yticklabels(CLASSES, fontdict={'fontsize': 7})\n    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    titlestring = \"\"\n    if score is not None:\n        titlestring += 'f1 = {:.3f} '.format(score)\n    if precision is not None:\n        titlestring += '\\nprecision = {:.3f} '.format(precision)\n    if recall is not None:\n        titlestring += '\\nrecall = {:.3f} '.format(recall)\n    if len(titlestring) > 0:\n        ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n    plt.show()\n    \ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:29:11.335061Z","iopub.execute_input":"2021-08-16T09:29:11.335585Z","iopub.status.idle":"2021-08-16T09:29:11.361802Z","shell.execute_reply.started":"2021-08-16T09:29:11.335548Z","shell.execute_reply":"2021-08-16T09:29:11.361053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datasets","metadata":{}},{"cell_type":"code","source":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)  # image format uint8 [0,255]\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum # returns a dataset of image(s)\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset\n\ndef data_augment(image, label):\n    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n    # of the TPU while the TPU itself is computing gradients.\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_hue(image, max_delta=0.2)\n    image =  tf.image.stateless_random_contrast(image, lower=0.1, upper=0.9, seed=(2, 0))\n    image = tf.image.random_crop(image, (448, 448, 3))\n    image = tf.image.random_saturation(image, 0, 2)\n    return image, label   \n\ndef test_resize(image, label):\n    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n    # of the TPU while the TPU itself is computing gradients.\n    image = tf.image.resize(image, (448, 448))\n    #image = tf.image.random_saturation(image, 0, 2)\n    return image, label\n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(PER_REPLICA_BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True,)\n    dataset = dataset.map(test_resize, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(PER_REPLICA_BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(PER_REPLICA_BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-08-16T09:29:15.516366Z","iopub.execute_input":"2021-08-16T09:29:15.516987Z","iopub.status.idle":"2021-08-16T09:29:15.536845Z","shell.execute_reply.started":"2021-08-16T09:29:15.516949Z","shell.execute_reply":"2021-08-16T09:29:15.536147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset visualizations","metadata":{}},{"cell_type":"code","source":"# data dump\nprint(\"Training data shapes:\")\nfor image, label in get_training_dataset().take(3):\n    print(image.numpy().shape, label.numpy().shape)\nprint(\"Training data label examples:\", label.numpy())\nprint(\"Validation data shapes:\")\nfor image, label in get_validation_dataset().take(3):\n    print(image.numpy().shape, label.numpy().shape)\nprint(\"Validation data label examples:\", label.numpy())\nprint(\"Test data shapes:\")\nfor image, idnum in get_test_dataset().take(3):\n    print(image.numpy().shape, idnum.numpy().shape)\nprint(\"Test data IDs:\", idnum.numpy().astype('U')) # U=unicode string","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:29:18.043625Z","iopub.execute_input":"2021-08-16T09:29:18.043981Z","iopub.status.idle":"2021-08-16T09:29:27.300831Z","shell.execute_reply.started":"2021-08-16T09:29:18.043945Z","shell.execute_reply":"2021-08-16T09:29:27.300157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Peek at training data\n# training_dataset = get_training_dataset()\n# training_dataset = training_dataset.unbatch().batch(20)\n# train_batch = iter(training_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:29:38.08191Z","iopub.execute_input":"2021-08-16T09:29:38.082419Z","iopub.status.idle":"2021-08-16T09:29:38.085596Z","shell.execute_reply.started":"2021-08-16T09:29:38.082387Z","shell.execute_reply":"2021-08-16T09:29:38.084801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run this cell again for next set of images\n# display_batch_of_images(next(train_batch))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:29:39.713352Z","iopub.execute_input":"2021-08-16T09:29:39.713873Z","iopub.status.idle":"2021-08-16T09:29:39.717217Z","shell.execute_reply.started":"2021-08-16T09:29:39.71384Z","shell.execute_reply":"2021-08-16T09:29:39.716252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# peer at test data\n# test_dataset = get_test_dataset()\n# test_dataset = test_dataset.unbatch().batch(20)\n# test_batch = iter(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:29:40.077197Z","iopub.execute_input":"2021-08-16T09:29:40.077608Z","iopub.status.idle":"2021-08-16T09:29:40.08163Z","shell.execute_reply.started":"2021-08-16T09:29:40.077572Z","shell.execute_reply":"2021-08-16T09:29:40.080507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run this cell again for next set of images\n# display_batch_of_images(next(test_batch))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:29:41.498558Z","iopub.execute_input":"2021-08-16T09:29:41.498906Z","iopub.status.idle":"2021-08-16T09:29:41.503114Z","shell.execute_reply.started":"2021-08-16T09:29:41.498877Z","shell.execute_reply":"2021-08-16T09:29:41.50196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\nNot the best but it converges ...","metadata":{}},{"cell_type":"code","source":"\"\"\"config\"\"\"\nCONV_KER_INIT = 'random_normal'\nCONV_BIAS_INIT = 'zeros'\nDENSE_KER_INIT = 'glorot_normal'\nDENSE_BIAS_INIT = 'zeros'\nBASE_FTS_SIZE = 28\nCLASSES_NUM = 104","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:36:54.919377Z","iopub.execute_input":"2021-08-16T09:36:54.919787Z","iopub.status.idle":"2021-08-16T09:36:54.925046Z","shell.execute_reply.started":"2021-08-16T09:36:54.919752Z","shell.execute_reply":"2021-08-16T09:36:54.923856Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"\"\"\"Route\"\"\"\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.regularizers as regularizers\n\nclass GlobalContext(keras.layers.Layer):\n    def __init__(self, inplanes, planes, size, channels, pool=None, fusion=None):\n        super(GlobalContext, self).__init__()\n        if pool is None:\n            pool = 'avg'\n        if fusion is None:\n            fusion = ['channel_add', 'channel_mul']\n        assert pool in ['avg', 'att']\n        assert all([f in ['channel_add', 'channel_mul'] for f in fusion])\n        assert len(fusion) > 0, 'fusion should be used'\n        self.inplanes = inplanes\n        self.planes = planes\n        self.pool = pool\n        self.fusion = fusion\n        self.size = size\n        self.channels = channels\n\n        if 'att' in pool:\n            self.conv_mask = keras.layers.Conv2D(\n                filters=1,\n                kernel_size=1,\n                strides=(1, 1),\n                kernel_initializer=CONV_KER_INIT,\n                bias_initializer=CONV_BIAS_INIT,\n                activation='relu',\n                kernel_regularizer=regularizers.l2(0.01)\n            )\n            self.softmax = keras.layers.Softmax(axis=-1)\n        else:\n            self.avg_pool = keras.layers.GlobalAveragePooling2D()\n        \n        if 'channel_add' in self.fusion:\n            self.channel_add_conv = keras.Sequential(\n                [\n                 keras.layers.LayerNormalization(axis=[1, 2, 3], epsilon=1e-5),\n                 keras.layers.ReLU(),\n                 keras.layers.Conv2D(\n                     filters=self.inplanes,\n                     kernel_size=1,\n                     strides=(1, 1),\n                     kernel_initializer=CONV_KER_INIT,\n                     bias_initializer=CONV_BIAS_INIT,\n                     activation='relu',\n                     kernel_regularizer=regularizers.l2(0.01)\n                 )\n                ]\n            )\n        else:\n            self.channel_add_conv = None\n        if 'channel_mul' in self.fusion:\n            self.channel_mul_conv = keras.Sequential(\n                [\n                 keras.layers.Conv2D(\n                     filters=self.planes,\n                     kernel_size=1,\n                     kernel_initializer=CONV_KER_INIT,\n                     bias_initializer=CONV_BIAS_INIT,\n                     activation='relu',\n                     kernel_regularizer=regularizers.l2(0.01)\n                 ),\n                 keras.layers.LayerNormalization(axis=[1, 2, 3], epsilon=1e-5),\n                 keras.layers.ReLU(),\n                 keras.layers.Conv2D(\n                     filters=self.inplanes,\n                     kernel_size=1,\n                     kernel_initializer=CONV_KER_INIT,\n                     bias_initializer=CONV_BIAS_INIT,\n                     activation='relu',\n                     kernel_regularizer=regularizers.l2(0.01)\n                 )\n                ]\n            )\n        else:\n            self.channel_mul_conv = None\n\n    def spatial_pool(self, x):\n        if self.pool == 'att':\n            input_x = x\n            input_x = tf.reshape(input_x, (-1, self.size*self.size, self.channels))\n            input_x = tf.transpose(input_x, [0, 2, 1])\n            input_x = tf.expand_dims(input_x, axis=1)\n\n            context_mask = self.conv_mask(x)\n            context_mask = tf.reshape(context_mask, (-1, self.size*self.size, 1))\n            context_mask = tf.transpose(context_mask, [0, 2, 1])\n            context_mask = self.softmax(context_mask)\n            context_mask = context_mask.unsequeeze(3)\n            context = torch.matmul(input_x, context_mask)\n            context = tf.reshape(context, [-1, self.channel, 1, 1])\n            context = tf.transpose(context, [0, 2, 3, 1])\n        else:\n            context = self.avg_pool(x)\n            context = tf.reshape(context, (-1, 1, 1, self.channels))\n\n        return context\n    \n    def call(self, inputs):\n        context = self.spatial_pool(inputs)\n\n        if self.channel_mul_conv is not None:\n            channel_mul_term = keras.activations.sigmoid(self.channel_mul_conv(context))\n            out = inputs * channel_mul_term\n        else:\n            out = inputs\n        if self.channel_add_conv is not None:\n            channel_add_term = self.channel_add_conv(context)\n            out = out + channel_add_term\n        return out\n\n\nclass Route(keras.layers.Layer):\n    def __init__(self, in_channels, size, epslion=1e-12):\n        super(Route, self).__init__()\n        self.conv1 = keras.layers.Conv2D(\n            filters = in_channels,\n            kernel_size = 1,\n            kernel_initializer = CONV_KER_INIT,\n            bias_initializer = CONV_BIAS_INIT,\n            kernel_regularizer=regularizers.l2(0.01),\n            activation='relu',\n        )\n        self.gcblock = GlobalContext(\n            inplanes = in_channels,\n            planes = in_channels,\n            size = size,\n            channels = in_channels\n        )\n        self.l2norm = tf.math.l2_normalize\n        self.fc = keras.layers.Dense(\n            units=1,\n            kernel_initializer=DENSE_KER_INIT, \n            bias_initializer=DENSE_BIAS_INIT,\n            kernel_regularizer=regularizers.l2(0.01)\n        )\n\n        self.avgpool = keras.layers.GlobalAveragePooling2D()\n        self.sigmoid = keras.activations.sigmoid\n        self.epslion = epslion\n\n    def call(self, inputs):\n        feature = self.conv1(inputs)\n        feature = self.gcblock(feature)\n        feature = self.avgpool(feature)\n        feature = tf.sign(feature)*tf.math.sqrt(tf.sign(feature)*feature+self.epslion)\n        feature = self.l2norm(feature, axis=-1)\n\n        out = self.fc(feature)\n        out = self.sigmoid(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:36:56.444204Z","iopub.execute_input":"2021-08-16T09:36:56.444575Z","iopub.status.idle":"2021-08-16T09:36:56.477163Z","shell.execute_reply.started":"2021-08-16T09:36:56.444541Z","shell.execute_reply":"2021-08-16T09:36:56.476129Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"\"\"\"attention\"\"\"\nclass ASPP(keras.layers.Layer):\n    def __init__(self, in_channels, depth, size):\n        super(ASPP, self).__init__()\n        self.size = size\n\n        self.mean = keras.layers.GlobalAveragePooling2D()\n        self.conv = keras.layers.Conv2D(\n            filters=depth,\n            kernel_size=1,\n            kernel_initializer=CONV_KER_INIT,\n            bias_initializer=CONV_BIAS_INIT,\n#             activation='relu',\n#             kernel_regularizer=regularizers.l2(0.01)\n        )\n        self.upsample = keras.layers.UpSampling2D(size, interpolation='bilinear')\n        self.atrous_block1 = keras.layers.Conv2D(\n            depth, \n            kernel_size=1, \n            strides=1, \n            kernel_initializer=CONV_KER_INIT,\n            bias_initializer=CONV_BIAS_INIT, \n            padding='same',\n#             activation='relu',\n#             kernel_regularizer=regularizers.l2(0.01)\n        )\n        self.atrous_block6 = keras.layers.Conv2D(\n            depth,\n            kernel_size=3,\n            strides=1,\n            kernel_initializer=CONV_KER_INIT,\n            bias_initializer=CONV_BIAS_INIT, \n            padding='same',\n            dilation_rate=6,\n#             activation='relu',\n#             kernel_regularizer=regularizers.l2(0.01)\n        )\n        self.atrous_block12 = keras.layers.Conv2D(\n            depth, \n            kernel_size=3,\n            strides=1, \n            kernel_initializer=CONV_KER_INIT, \n            bias_initializer=CONV_BIAS_INIT, \n            padding='same', \n            dilation_rate=12, \n#             activation='relu',\n#             kernel_regularizer=regularizers.l2(0.01)\n        )\n        self.atrous_block18 = keras.layers.Conv2D(\n            depth, \n            kernel_size=3, \n            strides=1,\n            kernel_initializer=CONV_KER_INIT,\n            bias_initializer=CONV_BIAS_INIT,\n            padding='same', \n            dilation_rate=18, \n#             activation='relu',\n#             kernel_regularizer=regularizers.l2(0.01)\n        )\n\n        self.conv_1_output = keras.layers.Conv2D(\n            depth,\n            kernel_size=1,\n            kernel_initializer=CONV_KER_INIT,\n            bias_initializer=CONV_BIAS_INIT,\n#             activation='relu',\n#             kernel_regularizer=regularizers.l2(0.01)\n        )\n    \n    def call(self, inputs):\n        img_features = self.mean(inputs)\n        img_features = tf.expand_dims(img_features, axis=1)\n        img_features = tf.expand_dims(img_features, axis=1)\n        img_features = self.conv(img_features)\n        img_features = self.upsample(img_features)\n\n        atrous_block1 = self.atrous_block1(inputs)\n        atrous_block6 = self.atrous_block6(inputs)\n        atrous_block12 = self.atrous_block12(inputs)\n        atrous_block18 = self.atrous_block18(inputs)\n\n        all_ = tf.concat([img_features, atrous_block1, atrous_block6, atrous_block12, atrous_block18], axis=-1)\n        net = self.conv_1_output(all_)\n        return net\n\nclass SEBlock(keras.layers.Layer):\n    def __init__(self, in_planes, planes, stride=1):\n        super(SEBlock, self).__init__()\n        self.conv1 = keras.layers.Conv2D(\n            filters=planes,\n            kernel_size=3,\n            strides=stride,\n            padding='same',\n            use_bias=False,\n            kernel_initializer=CONV_KER_INIT,\n            bias_initializer=CONV_BIAS_INIT,\n            activation='relu',\n            kernel_regularizer=regularizers.l2(0.01)\n        )\n        self.bn1 = keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n        self.conv2 = keras.layers.Conv2D(\n            filters=planes,\n            kernel_size=3,\n            strides=stride,\n            padding='same',\n            use_bias=False,\n            kernel_initializer=CONV_KER_INIT,\n            bias_initializer=CONV_BIAS_INIT,\n            activation='relu',\n            kernel_regularizer=regularizers.l2(0.01)\n        )\n        self.bn2 = keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n        self.avgpool = keras.layers.GlobalAveragePooling2D()\n        self.sigmoid = keras.activations.sigmoid\n\n        self.shortcut = keras.Sequential([])\n        if stride != 1 or in_planes != planes:\n            self.shortcut = keras.Sequential(\n                [\n                 keras.layers.Conv2D(\n                     filters=planes,\n                     kernel_size=1,\n                     stride=stride,\n                     use_bias=False,\n                     kernel_initializer=CONV_KER_INIT,\n                     bias_initializer=CONV_BIAS_INIT\n                 ),\n                 keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n                ]\n            )\n        \n        self.fc1 = keras.layers.Conv2D(\n            filters=planes//16,\n            kernel_size=1,\n            kernel_initializer=CONV_KER_INIT,\n            bias_initializer=CONV_BIAS_INIT,\n            activation='relu',\n            kernel_regularizer=regularizers.l2(0.01)\n        )\n        self.fc2 = keras.layers.Conv2D(\n            filters=planes,\n            kernel_size=1,\n            kernel_initializer=CONV_KER_INIT,\n            bias_initializer=CONV_BIAS_INIT,\n            activation='relu',\n            kernel_regularizer=regularizers.l2(0.01)\n        )\n\n    def call(self, inputs, training=None):\n        out = keras.activations.relu(self.bn1(self.conv1(inputs)))\n        out = self.bn2(self.conv2(out))\n\n        w = self.avgpool(out)\n        w = tf.expand_dims(w, axis=1)\n        w = tf.expand_dims(w, axis=1)\n        w = keras.activations.relu(self.fc1(w))\n        w = self.sigmoid(self.fc2(w))\n\n        out = out*w\n        out += self.shortcut(inputs)\n        out = keras.activations.relu(out)\n        return out\n\nclass Attention(keras.layers.Layer):\n    def __init__(self, in_channels):\n        super(Attention, self).__init__()\n        self.aspp = keras.Sequential()\n#         self.aspp = ASPP(in_channels=in_channels, depth=in_channels, size=BASE_FTS_SIZE)\n        self.se = SEBlock(in_planes=in_channels, planes=in_channels)\n    \n    def call(self, inputs):\n        feature = self.aspp(inputs)\n        out = self.se(feature)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:36:57.250894Z","iopub.execute_input":"2021-08-16T09:36:57.251242Z","iopub.status.idle":"2021-08-16T09:36:57.279761Z","shell.execute_reply.started":"2021-08-16T09:36:57.251211Z","shell.execute_reply":"2021-08-16T09:36:57.278526Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"\"\"\"LabelPred\"\"\"\nclass LabelPred(keras.layers.Layer):\n    def __init__(self, class_num, in_channels):\n        super(LabelPred, self).__init__()\n        self.class_num = class_num\n\n        self.bn = keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n        self.conv1 = keras.layers.Conv2D(\n            filters=in_channels,\n            kernel_size=1,\n            kernel_initializer=CONV_KER_INIT,\n            bias_initializer=CONV_BIAS_INIT,\n            activation='relu',\n            kernel_regularizer=regularizers.l2(0.01)\n        )\n        self.l2norm = tf.math.l2_normalize\n        self.dropout = keras.layers.Dropout(0.3)\n        self.fc = keras.layers.Dense(\n            units=self.class_num,\n            activation='softmax',\n            kernel_initializer=DENSE_KER_INIT,\n            bias_initializer=DENSE_BIAS_INIT,\n            kernel_regularizer=regularizers.l2(0.01)\n        )\n\n        self.avgpool = keras.layers.GlobalAveragePooling2D()\n    \n    def call(self, inputs, training=None):\n        feature = self.bn(inputs)\n        feature = self.conv1(feature)\n        feature = self.avgpool(feature)\n        feature = tf.sign(feature) * tf.math.sqrt(tf.sign(feature) * feature + 1e-12)\n        feature = self.l2norm(feature, axis=-1)\n        \n        feature =  self.dropout(feature)\n        feature = self.fc(feature)\n        return feature","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:56:10.718063Z","iopub.execute_input":"2021-08-16T09:56:10.718468Z","iopub.status.idle":"2021-08-16T09:56:10.728791Z","shell.execute_reply.started":"2021-08-16T09:56:10.718428Z","shell.execute_reply":"2021-08-16T09:56:10.727941Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"\"\"\"Tree\"\"\"\nclass BTree(keras.layers.Layer):\n    def __init__(self, in_channel, classes, size):\n        super(BTree, self).__init__()\n        self.route1_L1 = Route(in_channel, size)\n        self.route1_L2 = Route(in_channel, size)\n        self.route2_L2 = Route(in_channel, size)\n\n        self.att_1 = Attention(in_channel)\n        self.att_2 = Attention(in_channel)\n        self.att_3 = Attention(in_channel)\n\n        self.att_4 = Attention(in_channel)\n        self.att_6 = Attention(in_channel)\n        self.att_8 = Attention(in_channel)\n\n        self.att_5 = Attention(in_channel)\n        self.att_7 = Attention(in_channel)\n        self.att_9 = Attention(in_channel)\n\n        self.p_1 = LabelPred(classes, in_channel)\n        self.p_2 = LabelPred(classes, in_channel)\n        self.p_3 = LabelPred(classes, in_channel)\n        self.p_4 = LabelPred(classes, in_channel)\n\n    def call(self, inputs):\n        \"\"\"为了清楚搭建树结构，未使用任何循环简化代码，本树结构完全按照论文figure2实现\"\"\"\n        route_result = [[0, 0], [0, 0, 0, 0]]\n        features_result = [[None], [None, None], [None, None, None, None]]\n\n        # 第一层Route\n        left_prob1_l1 = self.route1_L1(inputs)\n        right_prob1_l1 = 1 - left_prob1_l1\n        route_result[0][0], route_result[0][1] = left_prob1_l1, right_prob1_l1\n\n        features_result[0][0] = inputs\n        # 第一层attention\n        left_fts1_l1 = self.att_3(self.att_1(inputs))\n        right_fts1_l1 = self.att_2(inputs)\n        features_result[1][0], features_result[1][1] = left_fts1_l1, right_fts1_l1\n\n        # 第二层Route\n        left_prob1_l2 = self.route1_L2(features_result[1][0])\n        right_prob1_l2 = 1 - left_prob1_l2\n        left_prob2_l2 = self.route2_L2(features_result[1][1])\n        right_prob2_l2 = 1 - left_prob2_l2\n        route_result[1][0], route_result[1][1] = left_prob1_l2 * route_result[0][0], right_prob1_l2 * route_result[0][0]\n        route_result[1][2], route_result[1][3] = left_prob2_l2 * route_result[0][1], right_prob2_l2 * route_result[0][1]\n\n        # 第二层attention\n        left_fts1_l2 = self.att_8(self.att_4(features_result[1][0]))\n        right_fts1_l2 = self.att_6(features_result[1][0])\n        features_result[2][0], features_result[2][1] = left_fts1_l2, right_fts1_l2\n\n        left_fts2_l2 = self.att_9(self.att_5(features_result[1][1]))\n        right_fts2_l2 = self.att_7(features_result[1][1])\n        features_result[2][2], features_result[2][3] = left_fts2_l2, right_fts2_l2\n\n        # label pred\n        labelPred1 = self.p_1(features_result[2][0])\n        AvgLabel = (labelPred1 * route_result[1][0])\n\n        labelPred2 = self.p_2(features_result[2][1])\n        AvgLabel += (labelPred2 * route_result[1][1])\n\n        labelPred3 = self.p_3(features_result[2][2])\n        AvgLabel += (labelPred3 * route_result[1][2])\n\n        labelPred4 = self.p_4(features_result[2][3])\n        AvgLabel += (labelPred4 * route_result[1][3])\n\n        return labelPred1, labelPred2, labelPred3, labelPred4, AvgLabel\n#         return labelPred1, labelPred2, AvgLabel\n","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:56:12.410757Z","iopub.execute_input":"2021-08-16T09:56:12.411358Z","iopub.status.idle":"2021-08-16T09:56:12.429974Z","shell.execute_reply.started":"2021-08-16T09:56:12.411321Z","shell.execute_reply":"2021-08-16T09:56:12.429274Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"\"\"\"acnet\"\"\"\ndef ACnet(input_shape, in_channel, classes, size, firstStage=True):\n    backbone = keras.applications.ResNet50(\n        include_top=False,\n        weights='imagenet',\n        input_shape=input_shape,\n    )\n    if firstStage:\n        backbone.trainable = False\n    tree = BTree(in_channel=in_channel, classes=classes, size=size)\n\n    my_input = backbone.layers[0].input\n    output = backbone.get_layer('conv4_block6_out').output\n    output = keras.layers.Conv2D(\n        filters = in_channel,\n        kernel_size = 1,\n        kernel_initializer=CONV_KER_INIT,\n        activation='relu',\n        kernel_regularizer=regularizers.l2(0.01)\n    )(output)\n    output = tree(output)\n    all_model = keras.Model(inputs=my_input, outputs=output)\n    return all_model","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:56:13.834742Z","iopub.execute_input":"2021-08-16T09:56:13.835325Z","iopub.status.idle":"2021-08-16T09:56:13.842144Z","shell.execute_reply.started":"2021-08-16T09:56:13.835282Z","shell.execute_reply":"2021-08-16T09:56:13.841468Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"model = ACnet(input_shape=(448, 448, 3), in_channel=512, classes=104, size=28,firstStage=True)\nimg = tf.random.normal((2, 448, 448, 3))\ny = model(img)\nprint(y[0].shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:38:16.999214Z","iopub.execute_input":"2021-08-16T09:38:16.999564Z","iopub.status.idle":"2021-08-16T09:38:21.592643Z","shell.execute_reply.started":"2021-08-16T09:38:16.999534Z","shell.execute_reply":"2021-08-16T09:38:21.591832Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"(2, 104)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# loss","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    class myLoss(keras.losses.Loss):\n        def __init__(self, alpha=0.25, betha=1.):\n            super(myLoss, self).__init__(reduction=tf.keras.losses.Reduction.NONE)\n            # self.loss = SparseCategoricalCrossentropy(from_logits=False)\n            self.alpha = alpha\n            self.betha = betha\n\n        def call(self, y_true, y_pred):\n            a = self.NLLLoss(y_true, y_pred[0]) * self.alpha\n            b = self.NLLLoss(y_true, y_pred[1]) * self.alpha\n            c = self.NLLLoss(y_true, y_pred[2]) * self.alpha\n            d = self.NLLLoss(y_true, y_pred[3]) * self.alpha\n            e = self.NLLLoss(y_true, y_pred[4]) * self.betha\n            return a+b+c+d+e\n\n        def NLLLoss(self, y_true, y_pred):\n            y_true = tf.one_hot(y_true, CLASSES_NUM)\n            y_pred = tf.math.log(tf.math.add(y_pred, 0.000001))\n    #         loss = tf.einsum('mn, mn -> mn', y_true, y_pred)\n            loss = (y_true*y_pred)\n            loss = -(tf.reduce_sum(loss, axis=1))\n            return loss","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:38:24.636112Z","iopub.execute_input":"2021-08-16T09:38:24.636670Z","iopub.status.idle":"2021-08-16T09:38:24.645223Z","shell.execute_reply.started":"2021-08-16T09:38:24.636636Z","shell.execute_reply":"2021-08-16T09:38:24.644511Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"# training","metadata":{}},{"cell_type":"code","source":"NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\nVALIDATION_STEPS = (NUM_VALIDATION_IMAGES // BATCH_SIZE)-1 # The \"-(-//)\" trick rounds up instead of down :-)\nTEST_STEPS = NUM_TEST_IMAGES // BATCH_SIZE        # The \"-(-//)\" trick rounds up instead of down :-)\nPER_REPLICA_BATCH_SIZE = BATCH_SIZE // strategy.num_replicas_in_sync\nprint('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))\nprint(STEPS_PER_EPOCH, VALIDATION_STEPS)\n\nper_replica_batch_size = BATCH_SIZE // strategy.num_replicas_in_sync\ntrain_dataset = strategy.experimental_distribute_datasets_from_function(lambda _: get_training_dataset())\nval_dataset = strategy.experimental_distribute_datasets_from_function(lambda _: get_validation_dataset())\ntrain_iterator = iter(train_dataset)\nval_iterator = iter(val_dataset)\nlr_schedule = keras.optimizers.schedules.PiecewiseConstantDecay(\n    boundaries=[50, 100, 150],\n    values=[0.001, 0.0005, 0.0001, 0.00005]  # [0.5, 0.1, 0.01, 0.005]\n)\nwith strategy.scope():\n    model = ACnet(input_shape=(448, 448, 3), in_channel=512, classes=104, size=28,firstStage=True)\n#     model.load_weights('./model.h5')\n    loss_fn = myLoss(1, 1)\n    optimizer = tf.optimizers.Adam(learning_rate=lr_schedule)\n    training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('training_accuracy', dtype=tf.float32)\n    val_accuracy =tf.keras.metrics.SparseCategoricalAccuracy('val_accuracy', dtype=tf.float32)\n    training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n    \n\"\"\"speed up with tf.funcion\"\"\"\n@tf.function\ndef train_step(iterator):\n    def train_fn(inputs):\n        x, y = inputs\n        with tf.GradientTape() as tape:\n            logits = model(x, training=True)\n            loss_value = loss_fn(y, logits)\n            loss_value = tf.nn.compute_average_loss(loss_value, global_batch_size=BATCH_SIZE)\n\n        grads = tape.gradient(loss_value, model.trainable_weights)\n        optimizer.apply_gradients(list(zip(grads, model.trainable_weights)))\n        training_accuracy.update_state(y, logits[-1])\n        training_loss.update_state(loss_value * strategy.num_replicas_in_sync)\n    \n    strategy.run(train_fn, args=(next(iterator),))\n    \n@tf.function\ndef test_step(iterator):\n    def test_fn(inputs):\n        x, y = inputs\n        val_logits = model(x, training=False)\n        val_accuracy.update_state(y, val_logits[-1])\n    \n    strategy.run(test_fn, args=(next(iterator),))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:41:24.824307Z","iopub.execute_input":"2021-08-16T09:41:24.824696Z","iopub.status.idle":"2021-08-16T09:41:42.061966Z","shell.execute_reply.started":"2021-08-16T09:41:24.824663Z","shell.execute_reply":"2021-08-16T09:41:42.060843Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"Dataset: 12753 training images, 3712 validation images, 7382 unlabeled test images\n99 28\n","output_type":"stream"}]},{"cell_type":"markdown","source":"第一阶段的训练","metadata":{}},{"cell_type":"code","source":"epochs = 220\n\nfor epoch in range(epochs):\n    print('\\nstart of epoch %d'%(epoch,))\n    for step in range(STEPS_PER_EPOCH):\n        train_step(train_iterator)    \n    train_acc = training_accuracy.result()\n    print('training acc over epoch: %.4f, %4f'%(float(train_acc), float(training_loss.result())))\n    training_accuracy.reset_states()\n    training_loss.reset_states()\n\n    for step in range(VALIDATION_STEPS):\n        test_step(val_iterator)\n    val_acc = val_accuracy.result()\n    print('validation acc over epoch: %.4f'%(float(val_acc),))\n    val_accuracy.reset_states()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:38:48.135155Z","iopub.execute_input":"2021-08-16T09:38:48.135670Z","iopub.status.idle":"2021-08-16T09:39:04.839432Z","shell.execute_reply.started":"2021-08-16T09:38:48.135628Z","shell.execute_reply":"2021-08-16T09:39:04.838323Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"\nstart of epoch 0\nvalidation acc over epoch: 0.8728\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_weights('./model.h5')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:27:28.010843Z","iopub.status.idle":"2021-08-16T09:27:28.011252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"第二阶段的训练","metadata":{}},{"cell_type":"code","source":"lr_schedule_2 = keras.optimizers.schedules.PiecewiseConstantDecay(\n    boundaries=[50, 100, 150],\n    values=[0.00001, 0.05, 0.01, 0.005]  # [0.5, 0.1, 0.01, 0.005]\n)\nwith strategy.scope():\n    model = ACnet(input_shape=(448, 448, 3), in_channel=512, classes=104, size=28,firstStage=False)\n    model.load_weights('./model.h5')\n    loss_fn = myLoss(1, 1)\n    optimizer_2 = tf.optimizers.Adam(learning_rate=lr_schedule_2)\n    training_accuracy_2 = tf.keras.metrics.SparseCategoricalAccuracy('training_accuracy_2', dtype=tf.float32)\n    val_accuracy_2 =tf.keras.metrics.SparseCategoricalAccuracy('val_accuracy_2', dtype=tf.float32)\n    training_loss_2 = tf.keras.metrics.Mean('training_loss_2', dtype=tf.float32)\n    \n\"\"\"speed up with tf.funcion\"\"\"\n@tf.function\ndef train_step_2(iterator):\n    def train_fn(inputs):\n        x, y = inputs\n        with tf.GradientTape() as tape:\n            logits = model(x, training=True)\n            loss_value = loss_fn(y, logits)\n            loss_value = tf.nn.compute_average_loss(loss_value, global_batch_size=BATCH_SIZE)\n\n        grads = tape.gradient(loss_value, model.trainable_weights)\n        optimizer.apply_gradients(list(zip(grads, model.trainable_weights)))\n        training_accuracy_2.update_state(y, logits[-1])\n        training_loss_2.update_state(loss_value * strategy.num_replicas_in_sync)\n    \n    strategy.run(train_fn, args=(next(iterator),))\n    \n@tf.function\ndef test_step_2(iterator):\n    def test_fn(inputs):\n        x, y = inputs\n        val_logits = model(x, training=False)\n        val_accuracy_2.update_state(y, val_logits[-1])\n    \n    strategy.run(test_fn, args=(next(iterator),))\n    \nepochs = 150\nprint('stage two ......')\nfor epoch in range(epochs):\n    print('\\nstart of epoch %d'%(epoch,))\n    for step in range(STEPS_PER_EPOCH):\n        train_step_2(train_iterator)   \n    train_acc = training_accuracy_2.result()\n    print('training acc over epoch: %.4f, %4f'%(float(train_acc), float(training_loss_2.result())))\n    training_accuracy_2.reset_states()\n    training_loss_2.reset_states()\n\n    for step in range(VALIDATION_STEPS):\n        test_step_2(val_iterator)\n    val_acc = val_accuracy_2.result()\n    print('validation acc over epoch: %.4f'%(float(val_acc),))\n    val_accuracy_2.reset_states()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:56:22.749029Z","iopub.execute_input":"2021-08-16T09:56:22.749606Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"stage two ......\n\nstart of epoch 0\ntraining acc over epoch: 0.8780, 2.996365\nvalidation acc over epoch: 0.7902\n\nstart of epoch 1\ntraining acc over epoch: 0.9514, 1.559811\nvalidation acc over epoch: 0.8627\n\nstart of epoch 2\ntraining acc over epoch: 0.9694, 1.175860\nvalidation acc over epoch: 0.8680\n\nstart of epoch 3\ntraining acc over epoch: 0.9793, 0.955817\nvalidation acc over epoch: 0.8809\n\nstart of epoch 4\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_weights('./model_.h5')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:27:28.013461Z","iopub.status.idle":"2021-08-16T09:27:28.013886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with strategy.scope():\n#     #img_adjust_layer = tf.keras.layers.Lambda(lambda data: tf.keras.applications.xception.preprocess_input(tf.cast(data, tf.float32)), input_shape=[*IMAGE_SIZE, 3])\n#     #pretrained_model = tf.keras.applications.Xception(weights='imagenet', include_top=False)\n    \n#     img_adjust_layer = tf.keras.layers.Lambda(lambda data: tf.keras.applications.vgg16.preprocess_input(tf.cast(data, tf.float32)), input_shape=[*IMAGE_SIZE, 3])\n#     pretrained_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False)\n    \n#     pretrained_model.trainable = False # False = transfer learning, True = fine-tuning\n    \n#     model = tf.keras.Sequential([\n#         img_adjust_layer,\n#         pretrained_model,\n#         tf.keras.layers.GlobalAveragePooling2D(),\n#         tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n#     ])\n        \n# model.compile(\n#     optimizer='adam',\n#     loss = 'sparse_categorical_crossentropy',\n#     metrics=['sparse_categorical_accuracy'],\n#     # NEW on TPU in TensorFlow 24: sending multiple batches to the TPU at once saves communications\n#     # overheads and allows the XLA compiler to unroll the loop on TPU and optimize hardware utilization.\n#     steps_per_execution=16\n# )\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:27:28.01478Z","iopub.status.idle":"2021-08-16T09:27:28.015171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# history = model.fit(get_training_dataset(), steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS,\n#                     validation_data=get_validation_dataset(), validation_steps=VALIDATION_STEPS)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:27:28.016032Z","iopub.status.idle":"2021-08-16T09:27:28.016418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n# display_training_curves(history.history['sparse_categorical_accuracy'], history.history['val_sparse_categorical_accuracy'], 'accuracy', 212)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:27:28.017328Z","iopub.status.idle":"2021-08-16T09:27:28.017733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion matrix","metadata":{}},{"cell_type":"code","source":"# cmdataset = get_validation_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and labels, order matters.\n# images_ds = cmdataset.map(lambda image, label: image)\n# labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n# cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy() # get everything as one batch\n# cm_probabilities = model.predict(images_ds, steps=VALIDATION_STEPS)\n# cm_predictions = np.argmax(cm_probabilities, axis=-1)\n# print(\"Correct   labels: \", cm_correct_labels.shape, cm_correct_labels)\n# print(\"Predicted labels: \", cm_predictions.shape, cm_predictions)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:27:28.018478Z","iopub.status.idle":"2021-08-16T09:27:28.018868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cmat = confusion_matrix(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)))\n# score = f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n# precision = precision_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n# recall = recall_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n# cmat = (cmat.T / cmat.sum(axis=1)).T # normalized\n# display_confusion_matrix(cmat, score, precision, recall)\n# print('f1 score: {:.3f}, precision: {:.3f}, recall: {:.3f}'.format(score, precision, recall))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:27:28.019596Z","iopub.status.idle":"2021-08-16T09:27:28.020011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions","metadata":{}},{"cell_type":"code","source":"# test_ds = get_test_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and ids, order matters.\n\n# print('Computing predictions...')\n# test_images_ds = test_ds.map(lambda image, idnum: image)\n# probabilities = model.predict(test_images_ds, steps=TEST_STEPS)\n# predictions = np.argmax(probabilities, axis=-1)\n# print(predictions)\n\n# print('Generating submission.csv file...')\n# test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n# test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n# np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\n# !head submission.csv","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:27:28.020961Z","iopub.status.idle":"2021-08-16T09:27:28.021354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visual validation","metadata":{}},{"cell_type":"code","source":"# dataset = get_validation_dataset()\n# dataset = dataset.unbatch().batch(20)\n# batch = iter(dataset)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:27:28.022328Z","iopub.status.idle":"2021-08-16T09:27:28.022725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # run this cell again for next set of images\n# images, labels = next(batch)\n# probabilities = model.predict(tf.cast(images, tf.float32))\n# predictions = np.argmax(probabilities, axis=-1)\n# display_batch_of_images((images, labels), predictions)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:27:28.023666Z","iopub.status.idle":"2021-08-16T09:27:28.024059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}